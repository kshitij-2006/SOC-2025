# -*- coding: utf-8 -*-
"""Kshitij_assignment_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jyUzTc57c3VnQmvnqyfmGGEMgncWHRA1
"""

!pip install "numpy<2.0"

pip install torch torchvision Pillow dlib opencv-python tqdm matplotlib scipy

# Commented out IPython magic to ensure Python compatibility.
# Clone encoder4editing
!git clone https://github.com/omertov/encoder4editing.git
# %cd encoder4editing

# Install dependencies
!pip install torch==2.2.2 torchvision==0.17.2 ninja gdown

# Create model directory and download pretrained e4e model
!mkdir -p pretrained_models
!gdown 1cUv_reLE6k3604or78EranS7XzuVMWeO -O pretrained_models/e4e_ffhq_encode.pt

# Download facial landmark predictor
!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2
!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2

import torch
from torchvision import transforms
from PIL import Image
import sys
from argparse import Namespace
from models.psp import pSp
from utils.alignment import align_face
import dlib
import os

image_path="/content/photo.jpeg"

# Load model
ckpt = torch.load("pretrained_models/e4e_ffhq_encode.pt", map_location='cpu')
opts = ckpt['opts']
opts['checkpoint_path'] = "pretrained_models/e4e_ffhq_encode.pt"
opts = Namespace(**opts)
net = pSp(opts)
net.eval().cuda()

# Align face
predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
aligned_image = align_face(filepath=image_path, predictor=predictor)
aligned_image.save("aligned_face.jpg")

# Transform image
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])
img_tensor = transform(aligned_image).unsqueeze(0).cuda()

# Encode
with torch.no_grad():
    _, latent = net(img_tensor, randomize_noise=False, return_latents=True)

# Save latent vector
os.makedirs("outputs", exist_ok=True)
torch.save(latent[0].cpu(), "outputs/latent.pt")
print("âœ… Latent vector saved at outputs/latent.pt")

!ls

image_path = "/content/photo.jpeg"
img = Image.open(image_path)
display(img)  # Use this in Colab/Jupyter

"""Original Image Above"""

image_path = "/content/encoder4editing/aligned_face.jpg"
img = Image.open(image_path)
display(img)  # Use this in Colab/Jupyter

# Commented out IPython magic to ensure Python compatibility.
# coming back to content folder
# %cd '/content'

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git
!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3
# %cd stylegan2-ada-pytorch
!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl

import numpy as np

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

latent = torch.load('/content/latent.pt').to(device)  # Shape: [1, 512] or [512]
latent = latent.unsqueeze(0) if latent.dim() == 1 else latent  # Ensure shape [1, 512]

import dnnlib
import legacy
import torch
import numpy as np

with open('ffhq.pkl', 'rb') as f:
    G = legacy.load_network_pkl(f)['G_ema'].to(device)

latent = latent.unsqueeze(0)  # Add dimension at position 0
print(latent.shape)

# Assuming latent is [1, 18, 512] (W+ from e4e)
img_1 = G.synthesis(latent, noise_mode='const')  # Directly synthesize from W+

img_1 = (img_1.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)
img_1 = img_1[0].cpu().numpy()
from PIL import Image
Image.fromarray(img_1, 'RGB')

!wget https://github.com/genforce/interfacegan/blob/master/boundaries/stylegan_ffhq_smile_boundary.npy?raw=true -O smile_direction.npy

!wget "https://github.com/genforce/interfacegan/blob/master/boundaries/stylegan_ffhq_age_boundary.npy?raw=true" -O age_direction.npy

! rm age_direction.npy
!ls

!git clone https://github.com/genforce/interfacegan.git

!wget https://github.com/omertov/encoder4editing/raw/main/editings/interfacegan_directions/age.pt -P editings/interfacegan_directions/

!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd content
!ls

import numpy as np
import torch

smile_direction = torch.load("/content/smile.pt").to(device)
print("Direction shape:", smile_direction.shape)

print("Direction shape:", smile_direction.shape)
print(smile_direction)

print("Norm:", torch.norm(smile_direction))

# Assume latent is shape [1, 18, 512] (W+ space, StyleGAN2)
# direction is shape [512,]
import numpy as np
import torch

# Load smile direction
  # shape [512]
smile_direction = smile_direction.squeeze()  # shape becomes [512]
smile_direction = torch.tensor(smile_direction, device=device).float()

# Amount to modify: positive = more smiling, negative = less
intensity = -3.0  # or -2.0

# Apply to W+
latent_modified = latent.clone()
for i in range(8):
    latent_modified[:, i, :] += intensity * smile_direction

print("Direction shape:", smile_direction.shape)
print("Norm:", torch.norm(smile_direction))

with torch.no_grad():
    image = G.synthesis(latent_modified, noise_mode='const')  # shape [1, 3, 1024, 1024]

image = (image.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)
image = image[0].cpu().numpy()
from PIL import Image
Image.fromarray(image, 'RGB')